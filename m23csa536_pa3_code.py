------------------------------------------------------------------------------------
Github Link: https://github.com/M23CSA536-Codes/Speech_Understanding_Assignment_PA3
-------------------------------------------------------------------------------------


# -*- coding: utf-8 -*-
"""M23CSA536 PA3_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15SCuAnAWWfTXfhglu1aMHNWtEwsYHWrm

NAME - TORSHA CHATTERJEE

Roll No. - M23CSA536

Speech Understanding

Assignment - 3
"""

!pip install transformers peft accelerate --quiet

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import random
import numpy as np
from transformers import AutoModelForCausalLM

# Synthetic Gesture Dataset
class SyntheticGestureDataset(Dataset):
    def __init__(self, num_samples=500, gesture_dim=43, seed=0):
        random.seed(seed)
        torch.manual_seed(seed)
        self.samples = []
        for _ in range(num_samples):
            speech_feat = torch.randn(1, 512)  # Dummy speech embedding
            gesture = torch.randn(34, gesture_dim)  # 34 frames
            self.samples.append({"speech": speech_feat, "gestures": gesture})

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

class GestureVQVAE(nn.Module):
    def __init__(self, num_embeddings=1024, embedding_dim=512, input_dim=43):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, embedding_dim), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(embedding_dim, input_dim), nn.ReLU())
        self.codebook = nn.Embedding(num_embeddings, embedding_dim)

    def forward(self, x):
        # x: [batch, 34, 43]
        batch_size, seq_len, input_dim = x.shape
        x_flat = x.view(-1, input_dim)  # [batch*34, 43]
        z_e = self.encoder(x_flat)  # [batch*34, 512]
        codebook = self.codebook.weight  # [1024, 512]
        # Find closest codebook entries
        distances = ((z_e.unsqueeze(1) - codebook)**2).sum(-1)  # [batch*34, 1024]
        indices = torch.argmin(distances, dim=1)  # [batch*34]
        z_q = self.codebook(indices)  # [batch*34, 512]
        x_recon = self.decoder(z_q).view(batch_size, seq_len, input_dim)
        return x_recon, indices.view(batch_size, seq_len)  # (optional recon, tokens)

class GestureTranslator(nn.Module):
    def __init__(self, model_name, gesture_vocab_size):
        super().__init__()
        self.llm = AutoModelForCausalLM.from_pretrained(model_name)
        self.embedding_proj = nn.Embedding(gesture_vocab_size, self.llm.config.hidden_size)

    def forward(self, speech_embed, gesture_token_ids):
        # gesture_token_ids: [batch, seq_len]
        inputs_embeds = self.embedding_proj(gesture_token_ids)
        outputs = self.llm(inputs_embeds=inputs_embeds, labels=gesture_token_ids)
        return outputs.loss

def train(model, tokenizer, dataset, epochs=3, batch_size=4, lr=1e-4):
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            speech = batch["speech"]  # [B, 1, 512]
            gestures = batch["gestures"]  # [B, 34, 43]
            _, gesture_tokens = tokenizer(gestures)  # [B, 34]
            gesture_tokens = gesture_tokens.to(torch.long)

            loss = model(speech, gesture_tokens)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} - Avg Loss: {total_loss / len(dataloader):.4f}")

tokenizer = GestureVQVAE()
model_name = "gpt2"

print(" Training on TED Gesture (Synthetic)")
dataset1 = SyntheticGestureDataset(seed=1)
model1 = GestureTranslator(model_name, gesture_vocab_size=1024)
train(model1, tokenizer, dataset1)

print("\n Training on TED Expressive (Synthetic)")
dataset2 = SyntheticGestureDataset(seed=2)
model2 = GestureTranslator(model_name, gesture_vocab_size=1024)
train(model2, tokenizer, dataset2)

print("\n Training on Custom Dataset (Synthetic)")
dataset3 = SyntheticGestureDataset(seed=3)
model3 = GestureTranslator(model_name, gesture_vocab_size=1024)
train(model3, tokenizer, dataset3)

def compute_diversity(dataset):
    gestures = [sample["gestures"].numpy() for sample in dataset]
    gestures = np.stack(gestures)
    shuffled = gestures[np.random.permutation(len(gestures))]
    return np.mean(np.abs(gestures - shuffled))

print("\nðŸ§ª Diversity Scores:")
print("TED Gesture:", compute_diversity(dataset1))
print("TED Expressive:", compute_diversity(dataset2))
print("Custom:", compute_diversity(dataset3))
